{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":14891992,"datasetId":9528329,"databundleVersionId":15755929},{"sourceType":"datasetVersion","sourceId":14895179,"datasetId":9530328,"databundleVersionId":15759421}],"dockerImageVersionId":31287,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Enhanced Installation with latest versions\n!pip install transformers accelerate -q\n!pip install optuna scikit-learn torchmetrics -q\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enhanced device setup with mixed precision support\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"üöÄ Using device: {device}\")\n\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    # Enable cudnn benchmarking for faster training\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:04:23.105121Z","iopub.execute_input":"2026-02-20T04:04:23.105800Z","iopub.status.idle":"2026-02-20T04:04:53.809018Z","shell.execute_reply.started":"2026-02-20T04:04:23.105767Z","shell.execute_reply":"2026-02-20T04:04:53.808194Z"}},"outputs":[{"name":"stdout","text":"üöÄ Using device: cuda\nGPU: Tesla T4\nMemory: 15.64 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Detect environment\nIS_KAGGLE = os.path.exists('/kaggle/input')\nDATA_PATH = \"/kaggle/input/datasets/jasindavid/shifteddataset/label_shifted_fin_causality_dataset.csv\" if IS_KAGGLE else r\"D:\\NLP_ResearchPaper_work\\final_financial_causality_dataset.csv\"\n\nprint(f\"üìç Running on {'Kaggle' if IS_KAGGLE else 'Local'}\")\n\n# Load with enhanced parsing\ndf = pd.read_csv(DATA_PATH)\ndf[\"date\"] = pd.to_datetime(df[\"date\"])\n\n# Enhanced temporal split (80/20 with buffer to prevent data leakage)\nsplit_date = df[\"date\"].quantile(0.8)\ntrain_df = df[df[\"date\"] <= split_date].copy()\ntest_df = df[df[\"date\"] > split_date].copy()\n\n# Shuffle training data to prevent temporal bias\ntrain_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(f\"Train: {len(train_df)} | Test: {len(test_df)}\")\nprint(f\"Date range - Train: {train_df['date'].min()} to {train_df['date'].max()}\")\nprint(f\"Date range - Test: {test_df['date'].min()} to {test_df['date'].max()}\")\n\n# Check class distribution\nprint(\"\\nClass Distribution:\")\nprint(train_df[\"causal_label\"].value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:04:53.810826Z","iopub.execute_input":"2026-02-20T04:04:53.811255Z","iopub.status.idle":"2026-02-20T04:04:55.823815Z","shell.execute_reply.started":"2026-02-20T04:04:53.811219Z","shell.execute_reply":"2026-02-20T04:04:55.823055Z"}},"outputs":[{"name":"stdout","text":"üìç Running on Kaggle\nTrain: 29442 | Test: 7230\nDate range - Train: 2000-11-27 00:00:00 to 2018-04-26 00:00:00\nDate range - Test: 2018-04-27 00:00:00 to 2025-04-30 00:00:00\n\nClass Distribution:\ncausal_label\n1    0.78551\n0    0.21449\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Calculate class weights using effective number sampling (better for imbalanced data)\ntrain_counts = train_df[\"causal_label\"].value_counts().sort_index()\ntotal_samples = len(train_df)\n\n# Effective number weighting (reduces impact of majority class more aggressively)\nbeta = 0.9999\neffective_num = 1.0 - np.power(beta, train_counts.values)\nclass_weights = (1.0 - beta) / effective_num\nclass_weights = class_weights / class_weights.sum() * len(train_counts)\n\nprint(\"\\nüéØ Effective Number Class Weights:\")\nfor label, weight in zip(train_counts.index, class_weights):\n    print(f\"Class {label}: {weight:.4f} (n={train_counts[label]})\")\n\nweights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n# Create weighted sampler for balanced batches\nsample_weights = train_df[\"causal_label\"].map(lambda x: class_weights[x]).values\nsampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(train_df), replacement=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:04:55.824820Z","iopub.execute_input":"2026-02-20T04:04:55.825071Z","iopub.status.idle":"2026-02-20T04:04:56.117561Z","shell.execute_reply.started":"2026-02-20T04:04:55.825050Z","shell.execute_reply":"2026-02-20T04:04:56.116978Z"}},"outputs":[{"name":"stdout","text":"\nüéØ Effective Number Class Weights:\nClass 0: 1.3161 (n=6315)\nClass 1: 0.6839 (n=23127)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"MODEL_NAME = \"ProsusAI/finbert\"\n\n# Enhanced tokenizer with financial domain specifics\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Load base model with gradient checkpointing for memory efficiency\nbase_model = AutoModel.from_pretrained(MODEL_NAME)\nbase_model.gradient_checkpointing_enable()  # Saves memory, allows larger batch sizes\n\nprint(f\"‚úÖ Loaded {MODEL_NAME}\")\nprint(f\"Vocab size: {tokenizer.vocab_size}\")\nprint(f\"Max position embeddings: {base_model.config.max_position_embeddings}\")\n\n# Test tokenization on sample\nsample_text = train_df[\"clean_text\"].iloc[0]\ntokens = tokenizer(sample_text, truncation=True, max_length=256)\nprint(f\"\\nSample tokenization length: {len(tokens['input_ids'])}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:04:56.118422Z","iopub.execute_input":"2026-02-20T04:04:56.118703Z","iopub.status.idle":"2026-02-20T04:04:59.385796Z","shell.execute_reply.started":"2026-02-20T04:04:56.118671Z","shell.execute_reply":"2026-02-20T04:04:59.385029Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0601c1628162458c89b74afc63c8fba3"}},"metadata":{}},{"name":"stderr","text":"Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2781e593ec4c441ca76be86330b41a3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3166ea21f5674468ae7ef50654454c54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f142a1f0e3640a6892f3a24258c9347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea12f7b812c74f6d86b0a6cc690e70db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55114cc30470404b86d4aa50f894aed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"470a7573948e4b3e80278c52bff25f80"}},"metadata":{}},{"name":"stderr","text":"\u001b[1mBertModel LOAD REPORT\u001b[0m from: ProsusAI/finbert\nKey                          | Status     |  | \n-----------------------------+------------+--+-\nclassifier.weight            | UNEXPECTED |  | \nbert.embeddings.position_ids | UNEXPECTED |  | \nclassifier.bias              | UNEXPECTED |  | \n\n\u001b[3mNotes:\n- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Loaded ProsusAI/finbert\nVocab size: 30522\nMax position embeddings: 512\n\nSample tokenization length: 256\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class AdvancedTextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len=256, augment=False):\n        self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n        self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.augment = augment\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        # Simple text augmentation for minority class (random word deletion simulation via truncation)\n        if self.augment and self.labels[idx] == 1 and np.random.random() > 0.5:\n            words = text.split()\n            if len(words) > 10:\n                # Randomly truncate to simulate augmentation\n                keep_ratio = np.random.uniform(0.8, 1.0)\n                text = ' '.join(words[:int(len(words) * keep_ratio)])\n        \n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\nclass AdvancedMultimodalDataset(Dataset):\n    def __init__(self, texts, numerical, labels, tokenizer, max_len=256, augment=False):\n        self.texts = texts.tolist() if hasattr(texts, 'tolist') else texts\n        self.numerical = numerical\n        self.labels = labels.tolist() if hasattr(labels, 'tolist') else labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.augment = augment\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        \n        # Augmentation for minority class\n        if self.augment and self.labels[idx] == 1 and np.random.random() > 0.5:\n            words = text.split()\n            if len(words) > 10:\n                keep_ratio = np.random.uniform(0.8, 1.0)\n                text = ' '.join(words[:int(len(words) * keep_ratio)])\n        \n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_len,\n            return_tensors=\"pt\"\n        )\n        \n        return {\n            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n            \"numerical\": torch.tensor(self.numerical[idx], dtype=torch.float),\n            \"label\": torch.tensor(self.labels[idx], dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:04:59.387400Z","iopub.execute_input":"2026-02-20T04:04:59.387691Z","iopub.status.idle":"2026-02-20T04:04:59.399714Z","shell.execute_reply.started":"2026-02-20T04:04:59.387666Z","shell.execute_reply":"2026-02-20T04:04:59.398959Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class AttentionPooling(nn.Module):\n    \"\"\"Learned attention pooling for better sequence representation\"\"\"\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.Tanh(),\n            nn.Linear(hidden_size // 2, 1)\n        )\n    \n    def forward(self, hidden_states, attention_mask):\n        # hidden_states: [batch, seq_len, hidden]\n        scores = self.attention(hidden_states).squeeze(-1)  # [batch, seq_len]\n        scores = scores.masked_fill(~attention_mask.bool(), float('-inf'))\n        weights = torch.softmax(scores, dim=1).unsqueeze(-1)  # [batch, seq_len, 1]\n        pooled = (hidden_states * weights).sum(dim=1)  # [batch, hidden]\n        return pooled\n\nclass EnhancedFinBERTClassifier(nn.Module):\n    def __init__(self, base_model, dropout_rate=0.3):\n        super().__init__()\n        self.bert = base_model\n        self.hidden_size = base_model.config.hidden_size\n        \n        # Multi-head attention pooling instead of just CLS token\n        self.attention_pool = AttentionPooling(self.hidden_size)\n        \n        # Layer normalization for stability\n        self.layer_norm = nn.LayerNorm(self.hidden_size)\n        \n        # Deeper classifier with residual connections\n        self.classifier = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size // 2),\n            nn.LayerNorm(self.hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(self.hidden_size // 2, self.hidden_size // 4),\n            nn.LayerNorm(self.hidden_size // 4),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(self.hidden_size // 4, 2)\n        )\n        \n        # Initialize weights\n        self._init_weights()\n    \n    def _init_weights(self):\n        for module in self.classifier:\n            if isinstance(module, nn.Linear):\n                nn.init.xavier_uniform_(module.weight)\n                nn.init.zeros_(module.bias)\n    \n    def forward(self, input_ids, attention_mask):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        \n        # Use attention pooling over all tokens\n        pooled = self.attention_pool(outputs.last_hidden_state, attention_mask)\n        pooled = self.layer_norm(pooled)\n        \n        return self.classifier(pooled)\n\nclass EnhancedMultimodalFinBERT(nn.Module):\n    def __init__(self, base_model, num_numerical=3, dropout_rate=0.3):\n        super().__init__()\n        self.bert = base_model\n        self.hidden_size = base_model.config.hidden_size\n        \n        # Attention pooling\n        self.attention_pool = AttentionPooling(self.hidden_size)\n        self.layer_norm = nn.LayerNorm(self.hidden_size)\n        \n        # Enhanced numerical processing with deeper layers\n        self.num_processor = nn.Sequential(\n            nn.Linear(num_numerical, 64),\n            nn.LayerNorm(64),\n            nn.GELU(),\n            nn.Dropout(dropout_rate / 2),\n            nn.Linear(64, 32),\n            nn.LayerNorm(32),\n            nn.GELU()\n        )\n        \n        # Fusion layer with gating mechanism\n        self.fusion_gate = nn.Sequential(\n            nn.Linear(self.hidden_size + 32, self.hidden_size + 32),\n            nn.Sigmoid()\n        )\n        \n        # Classifier\n        combined_size = self.hidden_size + 32\n        self.classifier = nn.Sequential(\n            nn.Linear(combined_size, combined_size // 2),\n            nn.LayerNorm(combined_size // 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(combined_size // 2, combined_size // 4),\n            nn.LayerNorm(combined_size // 4),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(combined_size // 4, 2)\n        )\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        for module in [self.num_processor, self.classifier]:\n            for layer in module:\n                if isinstance(layer, nn.Linear):\n                    nn.init.xavier_uniform_(layer.weight)\n                    nn.init.zeros_(layer.bias)\n    \n    def forward(self, input_ids, attention_mask, numerical):\n        # Text branch\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = self.attention_pool(outputs.last_hidden_state, attention_mask)\n        text_features = self.layer_norm(text_features)\n        \n        # Numerical branch\n        num_features = self.num_processor(numerical)\n        \n        # Fusion with gating\n        combined = torch.cat((text_features, num_features), dim=1)\n        gate = self.fusion_gate(combined)\n        combined = combined * gate  # Gated fusion\n        \n        return self.classifier(combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:04:59.400848Z","iopub.execute_input":"2026-02-20T04:04:59.401132Z","iopub.status.idle":"2026-02-20T04:04:59.566552Z","shell.execute_reply.started":"2026-02-20T04:04:59.401111Z","shell.execute_reply":"2026-02-20T04:04:59.565655Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Text-only datasets with augmentation for training\ntrain_text_ds = AdvancedTextDataset(\n    train_df[\"clean_text\"], \n    train_df[\"causal_label\"], \n    tokenizer, \n    max_len=256,\n    augment=True  # Enable augmentation for minority class\n)\ntest_text_ds = AdvancedTextDataset(\n    test_df[\"clean_text\"], \n    test_df[\"causal_label\"], \n    tokenizer, \n    max_len=256,\n    augment=False\n)\n\n# Use weighted sampler for balanced training\ntrain_text_loader = DataLoader(\n    train_text_ds, \n    batch_size=32,  # Increased batch size due to gradient checkpointing\n    sampler=sampler,  # Use weighted sampler instead of shuffle\n    num_workers=2,\n    pin_memory=True\n)\ntest_text_loader = DataLoader(\n    test_text_ds, \n    batch_size=64, \n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\n# Enhanced numerical scaling using RobustScaler (better for financial outliers)\nscaler = RobustScaler()\nnum_train = scaler.fit_transform(train_df[[\"return_t1\", \"return_t5\", \"volatility_5\"]])\nnum_test = scaler.transform(test_df[[\"return_t1\", \"return_t5\", \"volatility_5\"]])\n\n# Multimodal datasets\ntrain_mm_ds = AdvancedMultimodalDataset(\n    train_df[\"clean_text\"], \n    num_train, \n    train_df[\"causal_label\"], \n    tokenizer,\n    max_len=256,\n    augment=True\n)\ntest_mm_ds = AdvancedMultimodalDataset(\n    test_df[\"clean_text\"], \n    num_test, \n    test_df[\"causal_label\"], \n    tokenizer,\n    max_len=256,\n    augment=False\n)\n\ntrain_mm_loader = DataLoader(\n    train_mm_ds, \n    batch_size=32, \n    sampler=sampler,\n    num_workers=2,\n    pin_memory=True\n)\ntest_mm_loader = DataLoader(\n    test_mm_ds, \n    batch_size=64, \n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\nprint(f\"‚úÖ DataLoaders ready\")\nprint(f\"Text batches: {len(train_text_loader)} | MM batches: {len(train_mm_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:04:59.567559Z","iopub.execute_input":"2026-02-20T04:04:59.568563Z","iopub.status.idle":"2026-02-20T04:04:59.602315Z","shell.execute_reply.started":"2026-02-20T04:04:59.568539Z","shell.execute_reply":"2026-02-20T04:04:59.601453Z"}},"outputs":[{"name":"stdout","text":"‚úÖ DataLoaders ready\nText batches: 921 | MM batches: 921\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class EarlyStopping:\n    def __init__(self, patience=3, min_delta=0.001, mode='max'):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.mode = mode\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        \n    def __call__(self, score):\n        if self.best_score is None:\n            self.best_score = score\n        elif self._is_improvement(score):\n            self.best_score = score\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.early_stop = True\n        return self.early_stop\n    \n    def _is_improvement(self, score):\n        if self.mode == 'max':\n            return score > self.best_score + self.min_delta\n        return score < self.best_score - self.min_delta\n\ndef train_model_enhanced(model, loader, val_loader, epochs=10, class_weights=None, model_name=\"model\"):\n    model.to(device)\n    \n    # Discriminative learning rates\n    no_decay = ['bias', 'LayerNorm.weight']\n    bert_params = list(model.bert.named_parameters())\n    classifier_params = []\n    \n    if hasattr(model, 'classifier'):\n        classifier_params += list(model.classifier.parameters())\n    if hasattr(model, 'num_processor'):\n        classifier_params += list(model.num_processor.parameters())\n    if hasattr(model, 'attention_pool'):\n        classifier_params += list(model.attention_pool.parameters())\n    if hasattr(model, 'fusion_gate'):\n        classifier_params += list(model.fusion_gate.parameters())\n    \n    optimizer_grouped_parameters = [\n        {\n            'params': [p for n, p in bert_params if not any(nd in n for nd in no_decay)],\n            'weight_decay': 0.01,\n            'lr': 1e-5\n        },\n        {\n            'params': [p for n, p in bert_params if any(nd in n for nd in no_decay)],\n            'weight_decay': 0.0,\n            'lr': 1e-5\n        },\n        {\n            'params': classifier_params,\n            'weight_decay': 0.01,\n            'lr': 5e-5\n        }\n    ]\n    \n    optimizer = AdamW(optimizer_grouped_parameters)\n    \n    # Scheduler\n    num_training_steps = len(loader) * epochs\n    num_warmup_steps = int(0.1 * num_training_steps)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=num_warmup_steps,\n        num_training_steps=num_training_steps\n    )\n    \n    # Loss with label smoothing\n    if class_weights is not None:\n        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n    else:\n        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    \n    # Mixed precision\n    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n    \n    early_stopping = EarlyStopping(patience=3, mode='max')\n    best_f1 = 0.0\n    best_model_state = None\n    \n    history = {'train_loss': [], 'val_f1': []}\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        total_loss = 0\n        progress_bar = tqdm(loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n        \n        for batch in progress_bar:\n            optimizer.zero_grad()\n            \n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"label\"].to(device)\n            \n            # Mixed precision\n            if scaler:\n                with torch.cuda.amp.autocast():\n                    if \"numerical\" in batch:\n                        numerical = batch[\"numerical\"].to(device)\n                        outputs = model(input_ids, attention_mask, numerical)\n                    else:\n                        outputs = model(input_ids, attention_mask)\n                    loss = criterion(outputs, labels)\n                \n                scaler.scale(loss).backward()\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                scaler.step(optimizer)\n                scaler.update()\n            else:\n                if \"numerical\" in batch:\n                    numerical = batch[\"numerical\"].to(device)\n                    outputs = model(input_ids, attention_mask, numerical)\n                else:\n                    outputs = model(input_ids, attention_mask)\n                loss = criterion(outputs, labels)\n                \n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n                optimizer.step()\n            \n            scheduler.step()\n            total_loss += loss.item()\n            progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n        \n        avg_loss = total_loss / len(loader)\n        history['train_loss'].append(avg_loss)\n        \n        # Validation - FIXED: Now returns dict directly\n        val_metrics = evaluate_model_enhanced(model, val_loader)\n        val_f1 = val_metrics['f1']  # This will work now!\n        history['val_f1'].append(val_f1)\n        \n        print(f\"\\nüìä Epoch {epoch+1} | Loss: {avg_loss:.4f} | Val F1: {val_f1:.4f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n        \n        # Save best model\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_model_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n            print(f\"‚úÖ New best model saved (F1: {best_f1:.4f})\")\n        \n        # Early stopping\n        if early_stopping(val_f1):\n            print(f\"‚èπÔ∏è Early stopping triggered at epoch {epoch+1}\")\n            break\n    \n    # Load best model\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n        print(f\"\\nüèÜ Loaded best model with F1: {best_f1:.4f}\")\n    \n    return model, history\n\nprint(\"‚úÖ Fixed training function loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:37:28.661119Z","iopub.execute_input":"2026-02-20T04:37:28.661754Z","iopub.status.idle":"2026-02-20T04:37:28.679728Z","shell.execute_reply.started":"2026-02-20T04:37:28.661727Z","shell.execute_reply":"2026-02-20T04:37:28.679008Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Fixed training function loaded\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# FIXED: Evaluation function now returns dictionary properly\ndef evaluate_model_enhanced(model, loader):\n    model.eval()\n    all_preds, all_probs, all_true = [], [], []\n    \n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            \n            if \"numerical\" in batch:\n                numerical = batch[\"numerical\"].to(device)\n                outputs = model(input_ids, attention_mask, numerical)\n            else:\n                outputs = model(input_ids, attention_mask)\n            \n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            \n            all_preds.extend(preds)\n            all_probs.extend(probs.cpu().numpy())\n            all_true.extend(batch[\"label\"].numpy())\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(all_true, all_preds),\n        'precision': precision_score(all_true, all_preds, average='binary', zero_division=0),\n        'recall': recall_score(all_true, all_preds, average='binary', zero_division=0),\n        'f1': f1_score(all_true, all_preds, average='binary', zero_division=0),\n        'auc': roc_auc_score(all_true, [p[1] for p in all_probs]) if len(set(all_true)) > 1 else 0.5\n    }\n    \n    return metrics  # Returns dict, not tuple!\n\n# For final evaluation when you need predictions back\ndef evaluate_model_full(model, loader):\n    model.eval()\n    all_preds, all_probs, all_true = [], [], []\n    \n    with torch.no_grad():\n        for batch in loader:\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            \n            if \"numerical\" in batch:\n                numerical = batch[\"numerical\"].to(device)\n                outputs = model(input_ids, attention_mask, numerical)\n            else:\n                outputs = model(input_ids, attention_mask)\n            \n            probs = torch.softmax(outputs, dim=1)\n            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n            \n            all_preds.extend(preds)\n            all_probs.extend(probs.cpu().numpy())\n            all_true.extend(batch[\"label\"].numpy())\n    \n    metrics = {\n        'accuracy': accuracy_score(all_true, all_preds),\n        'precision': precision_score(all_true, all_preds, average='binary', zero_division=0),\n        'recall': recall_score(all_true, all_preds, average='binary', zero_division=0),\n        'f1': f1_score(all_true, all_preds, average='binary', zero_division=0),\n        'auc': roc_auc_score(all_true, [p[1] for p in all_probs]) if len(set(all_true)) > 1 else 0.5\n    }\n    \n    return metrics, all_preds, all_probs, all_true\n\nprint(\"‚úÖ Fixed evaluation functions loaded\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:37:33.438090Z","iopub.execute_input":"2026-02-20T04:37:33.438370Z","iopub.status.idle":"2026-02-20T04:37:33.449099Z","shell.execute_reply.started":"2026-02-20T04:37:33.438344Z","shell.execute_reply":"2026-02-20T04:37:33.448440Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Fixed evaluation functions loaded\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Continue from where you left off - reinitialize the model and train\nprint(\"üîÑ Reinitializing model and continuing training...\")\n\n# Re-initialize the multimodal model\nmodel_mm = EnhancedMultimodalFinBERT(base_model, num_numerical=3, dropout_rate=0.4)\n\n# Recreate data splits\nval_split = int(0.9 * len(train_mm_ds))\ntrain_mm_subset, val_mm_subset = torch.utils.data.random_split(\n    train_mm_ds, [val_split, len(train_mm_ds) - val_split]\n)\n\n# Create loaders\ntrain_mm_loader_split = DataLoader(\n    train_mm_subset, \n    batch_size=32, \n    sampler=WeightedRandomSampler(\n        [sample_weights[i] for i in train_mm_subset.indices], \n        len(train_mm_subset), \n        replacement=True\n    ),\n    num_workers=2,\n    pin_memory=True\n)\n\nval_mm_loader = DataLoader(\n    val_mm_subset, \n    batch_size=64, \n    shuffle=False,\n    num_workers=2,\n    pin_memory=True\n)\n\n# Train with fixed function\nmodel_mm, history_mm = train_model_enhanced(\n    model_mm, \n    train_mm_loader_split, \n    val_mm_loader,\n    epochs=10, \n    class_weights=weights_tensor,\n    model_name=\"Multimodal\"\n)\n\nprint(\"\\n‚úÖ Training completed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T04:38:05.833578Z","iopub.execute_input":"2026-02-20T04:38:05.834290Z","iopub.status.idle":"2026-02-20T05:56:16.733333Z","shell.execute_reply.started":"2026-02-20T04:38:05.834262Z","shell.execute_reply":"2026-02-20T05:56:16.732598Z"}},"outputs":[{"name":"stdout","text":"üîÑ Reinitializing model and continuing training...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:09<00:00,  1.93it/s, loss=0.1239]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 1 | Loss: 0.4637 | Val F1: 0.9611 | LR: 1.00e-05\n‚úÖ New best model saved (F1: 0.9611)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:07<00:00,  1.94it/s, loss=0.3245]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 2 | Loss: 0.2860 | Val F1: 0.9826 | LR: 9.70e-06\n‚úÖ New best model saved (F1: 0.9826)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:05<00:00,  1.95it/s, loss=0.1198]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 3 | Loss: 0.2520 | Val F1: 0.9926 | LR: 8.83e-06\n‚úÖ New best model saved (F1: 0.9926)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:05<00:00,  1.95it/s, loss=0.1265]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 4 | Loss: 0.2396 | Val F1: 0.9910 | LR: 7.50e-06\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:02<00:00,  1.96it/s, loss=0.3215]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 5 | Loss: 0.2341 | Val F1: 0.9943 | LR: 5.87e-06\n‚úÖ New best model saved (F1: 0.9943)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:02<00:00,  1.96it/s, loss=0.3254]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 6 | Loss: 0.2303 | Val F1: 0.9950 | LR: 4.13e-06\n‚úÖ New best model saved (F1: 0.9950)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:01<00:00,  1.97it/s, loss=0.3216]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 7 | Loss: 0.2298 | Val F1: 0.9954 | LR: 2.50e-06\n‚úÖ New best model saved (F1: 0.9954)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:01<00:00,  1.97it/s, loss=0.3214]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 8 | Loss: 0.2257 | Val F1: 0.9943 | LR: 1.17e-06\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:02<00:00,  1.96it/s, loss=0.3246]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 9 | Loss: 0.2270 | Val F1: 0.9945 | LR: 3.02e-07\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 829/829 [07:00<00:00,  1.97it/s, loss=0.1210]\n                                                           \r","output_type":"stream"},{"name":"stdout","text":"\nüìä Epoch 10 | Loss: 0.2279 | Val F1: 0.9950 | LR: 0.00e+00\n‚èπÔ∏è Early stopping triggered at epoch 10\n\nüèÜ Loaded best model with F1: 0.9954\n\n‚úÖ Training completed successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Final evaluation using the full evaluation function\ntest_metrics_mm, mm_preds, mm_probs, mm_true = evaluate_model_full(model_mm, test_mm_loader)\n\nprint(f\"\\n{'='*50}\")\nprint(f\"üéØ Enhanced Multimodal Results\")\nprint(f\"{'='*50}\")\nprint(f\"Accuracy:  {test_metrics_mm['accuracy']:.4f}\")\nprint(f\"Precision: {test_metrics_mm['precision']:.4f}\")\nprint(f\"Recall:    {test_metrics_mm['recall']:.4f}\")\nprint(f\"F1-Score:  {test_metrics_mm['f1']:.4f}\")\nprint(f\"AUC-ROC:   {test_metrics_mm['auc']:.4f}\")\nprint(f\"{'='*50}\")\n\nprint(\"\\nDetailed Classification Report:\")\nprint(classification_report(mm_true, mm_preds, target_names=['Non-Causal', 'Causal']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T05:56:36.384881Z","iopub.execute_input":"2026-02-20T05:56:36.385191Z","iopub.status.idle":"2026-02-20T05:58:26.414820Z","shell.execute_reply.started":"2026-02-20T05:56:36.385160Z","shell.execute_reply":"2026-02-20T05:58:26.414055Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nüéØ Enhanced Multimodal Results\n==================================================\nAccuracy:  0.8228\nPrecision: 0.8617\nRecall:    0.8824\nF1-Score:  0.8719\nAUC-ROC:   0.8962\n==================================================\n\nDetailed Classification Report:\n              precision    recall  f1-score   support\n\n  Non-Causal       0.73      0.69      0.71      2288\n      Causal       0.86      0.88      0.87      4942\n\n    accuracy                           0.82      7230\n   macro avg       0.80      0.79      0.79      7230\nweighted avg       0.82      0.82      0.82      7230\n\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"import torch\nimport os\nimport pickle\nimport json\nfrom datetime import datetime\n\n# Create save directory\nsave_dir = \"/kaggle/working/models\" if IS_KAGGLE else r\"D:\\NLP_ResearchPaper_work\\models\"\nos.makedirs(save_dir, exist_ok=True)\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# ============================================\n# SAVE MULTIMODAL MODEL - STATE DICT ONLY (WORKING METHOD)\n# ============================================\n\n# 1. Save model weights only (RECOMMENDED - avoids pickle errors)\nmm_state_path = os.path.join(save_dir, f\"multimodal_state_{timestamp}.pth\")\ntorch.save(model_mm.state_dict(), mm_state_path)\nprint(f\"‚úÖ Model weights saved: {mm_state_path}\")\n\n# 2. Save config needed to reconstruct model\nconfig = {\n    'hidden_size': model_mm.hidden_size,\n    'num_numerical': 3,\n    'dropout_rate': 0.4,\n    'base_model_name': \"ProsusAI/finbert\",\n    'timestamp': timestamp\n}\nconfig_path = os.path.join(save_dir, f\"multimodal_config_{timestamp}.json\")\nwith open(config_path, 'w') as f:\n    json.dump(config, f)\nprint(f\"‚úÖ Config saved: {config_path}\")\n\n# 3. Save scaler\nscaler_path = os.path.join(save_dir, f\"scaler_{timestamp}.pkl\")\nwith open(scaler_path, 'wb') as f:\n    pickle.dump(scaler, f)\nprint(f\"‚úÖ Scaler saved: {scaler_path}\")\n\n# 4. Save tokenizer\ntokenizer_path = os.path.join(save_dir, f\"tokenizer_{timestamp}\")\ntokenizer.save_pretrained(tokenizer_path)\nprint(f\"‚úÖ Tokenizer saved: {tokenizer_path}\")\n\nprint(f\"\\n{'='*60}\")\nprint(f\"üíæ ALL FILES SAVED IN: {save_dir}\")\nprint(f\"{'='*60}\")\nprint(\"Files to download:\")\nprint(f\"  1. multimodal_state_{timestamp}.pth (model weights)\")\nprint(f\"  2. multimodal_config_{timestamp}.json (config)\")\nprint(f\"  3. scaler_{timestamp}.pkl (numerical scaler)\")\nprint(f\"  4. tokenizer_{timestamp}/ folder (tokenizer)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T06:13:56.931207Z","iopub.execute_input":"2026-02-20T06:13:56.931820Z","iopub.status.idle":"2026-02-20T06:13:57.533465Z","shell.execute_reply.started":"2026-02-20T06:13:56.931786Z","shell.execute_reply":"2026-02-20T06:13:57.532749Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Model weights saved: /kaggle/working/models/multimodal_state_20260220_061356.pth\n‚úÖ Config saved: /kaggle/working/models/multimodal_config_20260220_061356.json\n‚úÖ Scaler saved: /kaggle/working/models/scaler_20260220_061356.pkl\n‚úÖ Tokenizer saved: /kaggle/working/models/tokenizer_20260220_061356\n\n============================================================\nüíæ ALL FILES SAVED IN: /kaggle/working/models\n============================================================\nFiles to download:\n  1. multimodal_state_20260220_061356.pth (model weights)\n  2. multimodal_config_20260220_061356.json (config)\n  3. scaler_20260220_061356.pkl (numerical scaler)\n  4. tokenizer_20260220_061356/ folder (tokenizer)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import torch\nimport os\nfrom datetime import datetime\n\n# Create save directory\nsave_dir = \"/kaggle/working/models\" if IS_KAGGLE else r\"D:\\NLP_ResearchPaper_work\\models\"\nos.makedirs(save_dir, exist_ok=True)\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n\n# ============================================\n# SAVE EVERYTHING IN ONE FILE\n# ============================================\n\n# Prepare complete checkpoint\ncheckpoint = {\n    # Model weights\n    'model_state_dict': model_mm.state_dict(),\n    \n    # Model architecture config\n    'hidden_size': model_mm.hidden_size,\n    'num_numerical': 3,\n    'dropout_rate': 0.4,\n    \n    # Training info\n    'history': history_mm if 'history_mm' in locals() else None,\n    \n    # Scaler\n    'scaler': scaler,\n    \n    # Tokenizer info\n    'tokenizer_name': \"ProsusAI/finbert\",\n    'max_len': 256,\n    \n    # Metadata\n    'timestamp': timestamp,\n    'model_type': 'EnhancedMultimodalFinBERT'\n}\n\n# Save single file\nsingle_model_path = os.path.join(save_dir, f\"complete_multimodal_model_{timestamp}.pth\")\ntorch.save(checkpoint, single_model_path)\n\nprint(f\"‚úÖ Complete model saved: {single_model_path}\")\nprint(f\"File size: {os.path.getsize(single_model_path) / (1024**2):.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-20T06:15:47.700104Z","iopub.execute_input":"2026-02-20T06:15:47.700433Z","iopub.status.idle":"2026-02-20T06:15:48.227579Z","shell.execute_reply.started":"2026-02-20T06:15:47.700409Z","shell.execute_reply":"2026-02-20T06:15:48.226279Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Complete model saved: /kaggle/working/models/complete_multimodal_model_20260220_061547.pth\nFile size: 422.86 MB\n","output_type":"stream"}],"execution_count":19}]}