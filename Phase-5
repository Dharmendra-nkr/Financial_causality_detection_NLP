{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"modelInstanceVersion","sourceId":757709,"databundleVersionId":15778027,"modelInstanceId":578773,"modelId":591108,"isSourceIdPinned":false}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"9003dea2","cell_type":"markdown","source":"# Phase 5: Ablation & Robustness Analysis\n## Systematic Evaluation of Model Components & Resilience\n\n**Objective**: Quantify *each component's contribution* through ablation studies and evaluate *model resilience* under perturbations, noise, and distribution shifts.\n\n### Ablation Studies\n1. Component Ablation (Attention Pooling, Fusion Gate, Numerical Branch)\n2. Feature Ablation (Individual numerical features)\n3. Architectural Ablation (Text-only vs Multimodal vs Numerical-only)\n\n### Robustness Tests\n4. Text Perturbation (word dropout, character noise, synonym swap)\n5. Numerical Perturbation (Gaussian noise at varying scales)\n6. Temporal Distribution Shift (performance across time windows)\n7. Adversarial Robustness (FGSM-style gradient attacks)\n8. Input Length Sensitivity","metadata":{}},{"id":"4be029dd","cell_type":"markdown","source":"## Section 1: Import Libraries & Load Trained Model","metadata":{}},{"id":"8b664185","cell_type":"code","source":"import os\nimport copy\nimport random\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom sklearn.metrics import (accuracy_score, precision_score, recall_score,\n                             f1_score, roc_auc_score, classification_report,\n                             confusion_matrix)\nfrom sklearn.preprocessing import RobustScaler\nfrom transformers import AutoTokenizer, AutoModel, get_cosine_schedule_with_warmup\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_mem / 1e9:.2f} GB\")","metadata":{},"outputs":[],"execution_count":null},{"id":"de35f397","cell_type":"markdown","source":"## Section 2: Model Architecture (Must Match Phase 3 Training)","metadata":{}},{"id":"709c3c34","cell_type":"code","source":"# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# Architecture definitions â€” identical to Phase 3\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass AttentionPooling(nn.Module):\n    \"\"\"Learned attention pooling over token hidden states.\"\"\"\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.attention = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size // 2),\n            nn.Tanh(),\n            nn.Linear(hidden_size // 2, 1)\n        )\n\n    def forward(self, hidden_states, attention_mask):\n        scores = self.attention(hidden_states).squeeze(-1)\n        scores = scores.masked_fill(~attention_mask.bool(), float('-inf'))\n        weights = torch.softmax(scores, dim=1).unsqueeze(-1)\n        pooled = (hidden_states * weights).sum(dim=1)\n        return pooled\n\n\nclass EnhancedMultimodalFinBERT(nn.Module):\n    \"\"\"Full multimodal model: FinBERT text + numerical features with gated fusion.\"\"\"\n    def __init__(self, base_model, num_numerical=3, dropout_rate=0.4):\n        super().__init__()\n        self.bert = base_model\n        self.hidden_size = base_model.config.hidden_size\n\n        self.attention_pool = AttentionPooling(self.hidden_size)\n        self.layer_norm = nn.LayerNorm(self.hidden_size)\n\n        self.num_processor = nn.Sequential(\n            nn.Linear(num_numerical, 64),\n            nn.LayerNorm(64),\n            nn.GELU(),\n            nn.Dropout(dropout_rate / 2),\n            nn.Linear(64, 32),\n            nn.LayerNorm(32),\n            nn.GELU()\n        )\n\n        self.fusion_gate = nn.Sequential(\n            nn.Linear(self.hidden_size + 32, self.hidden_size + 32),\n            nn.Sigmoid()\n        )\n\n        combined_size = self.hidden_size + 32\n        self.classifier = nn.Sequential(\n            nn.Linear(combined_size, combined_size // 2),\n            nn.LayerNorm(combined_size // 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(combined_size // 2, combined_size // 4),\n            nn.LayerNorm(combined_size // 4),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(combined_size // 4, 2)\n        )\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for module in [self.num_processor, self.classifier]:\n            for layer in module:\n                if isinstance(layer, nn.Linear):\n                    nn.init.xavier_uniform_(layer.weight)\n                    nn.init.zeros_(layer.bias)\n\n    def forward(self, input_ids, attention_mask, numerical):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = self.attention_pool(outputs.last_hidden_state, attention_mask)\n        text_features = self.layer_norm(text_features)\n\n        num_features = self.num_processor(numerical)\n\n        combined = torch.cat((text_features, num_features), dim=1)\n        gate = self.fusion_gate(combined)\n        combined = combined * gate\n\n        return self.classifier(combined)\n\n\n# â”€â”€ Text-only variant (for ablation) â”€â”€\nclass TextOnlyFinBERT(nn.Module):\n    \"\"\"Text-only model â€” no numerical features.\"\"\"\n    def __init__(self, base_model, dropout_rate=0.4):\n        super().__init__()\n        self.bert = base_model\n        self.hidden_size = base_model.config.hidden_size\n        self.attention_pool = AttentionPooling(self.hidden_size)\n        self.layer_norm = nn.LayerNorm(self.hidden_size)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(self.hidden_size, self.hidden_size // 2),\n            nn.LayerNorm(self.hidden_size // 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(self.hidden_size // 2, self.hidden_size // 4),\n            nn.LayerNorm(self.hidden_size // 4),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(self.hidden_size // 4, 2)\n        )\n\n    def forward(self, input_ids, attention_mask, numerical=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = self.attention_pool(outputs.last_hidden_state, attention_mask)\n        text_features = self.layer_norm(text_features)\n        return self.classifier(text_features)\n\n\n# â”€â”€ Numerical-only variant (for ablation) â”€â”€\nclass NumericalOnlyClassifier(nn.Module):\n    \"\"\"Numerical-only baseline â€” no text.\"\"\"\n    def __init__(self, num_numerical=3, dropout_rate=0.4):\n        super().__init__()\n        self.processor = nn.Sequential(\n            nn.Linear(num_numerical, 64),\n            nn.LayerNorm(64),\n            nn.GELU(),\n            nn.Dropout(dropout_rate / 2),\n            nn.Linear(64, 32),\n            nn.LayerNorm(32),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(32, 2)\n        )\n\n    def forward(self, input_ids=None, attention_mask=None, numerical=None):\n        return self.processor(numerical)\n\n\n# â”€â”€ No-Fusion-Gate variant â”€â”€\nclass MultimodalNoGate(nn.Module):\n    \"\"\"Multimodal model WITHOUT gated fusion â€” simple concatenation.\"\"\"\n    def __init__(self, base_model, num_numerical=3, dropout_rate=0.4):\n        super().__init__()\n        self.bert = base_model\n        self.hidden_size = base_model.config.hidden_size\n        self.attention_pool = AttentionPooling(self.hidden_size)\n        self.layer_norm = nn.LayerNorm(self.hidden_size)\n\n        self.num_processor = nn.Sequential(\n            nn.Linear(num_numerical, 64),\n            nn.LayerNorm(64),\n            nn.GELU(),\n            nn.Dropout(dropout_rate / 2),\n            nn.Linear(64, 32),\n            nn.LayerNorm(32),\n            nn.GELU()\n        )\n\n        combined_size = self.hidden_size + 32\n        self.classifier = nn.Sequential(\n            nn.Linear(combined_size, combined_size // 2),\n            nn.LayerNorm(combined_size // 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(combined_size // 2, combined_size // 4),\n            nn.LayerNorm(combined_size // 4),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(combined_size // 4, 2)\n        )\n\n    def forward(self, input_ids, attention_mask, numerical):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = self.attention_pool(outputs.last_hidden_state, attention_mask)\n        text_features = self.layer_norm(text_features)\n        num_features = self.num_processor(numerical)\n        combined = torch.cat((text_features, num_features), dim=1)  # No gate!\n        return self.classifier(combined)\n\n\n# â”€â”€ No-Attention-Pooling variant (CLS only) â”€â”€\nclass MultimodalCLSPooling(nn.Module):\n    \"\"\"Multimodal model using CLS token instead of attention pooling.\"\"\"\n    def __init__(self, base_model, num_numerical=3, dropout_rate=0.4):\n        super().__init__()\n        self.bert = base_model\n        self.hidden_size = base_model.config.hidden_size\n        self.layer_norm = nn.LayerNorm(self.hidden_size)\n\n        self.num_processor = nn.Sequential(\n            nn.Linear(num_numerical, 64),\n            nn.LayerNorm(64),\n            nn.GELU(),\n            nn.Dropout(dropout_rate / 2),\n            nn.Linear(64, 32),\n            nn.LayerNorm(32),\n            nn.GELU()\n        )\n\n        self.fusion_gate = nn.Sequential(\n            nn.Linear(self.hidden_size + 32, self.hidden_size + 32),\n            nn.Sigmoid()\n        )\n\n        combined_size = self.hidden_size + 32\n        self.classifier = nn.Sequential(\n            nn.Linear(combined_size, combined_size // 2),\n            nn.LayerNorm(combined_size // 2),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(combined_size // 2, combined_size // 4),\n            nn.LayerNorm(combined_size // 4),\n            nn.GELU(),\n            nn.Dropout(dropout_rate),\n            nn.Linear(combined_size // 4, 2)\n        )\n\n    def forward(self, input_ids, attention_mask, numerical):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = outputs.last_hidden_state[:, 0, :]  # CLS token only\n        text_features = self.layer_norm(text_features)\n        num_features = self.num_processor(numerical)\n        combined = torch.cat((text_features, num_features), dim=1)\n        gate = self.fusion_gate(combined)\n        combined = combined * gate\n        return self.classifier(combined)\n\n\nprint(\"All model architectures defined\")","metadata":{},"outputs":[],"execution_count":null},{"id":"418673bb","cell_type":"markdown","source":"## Section 3: Load Data & Pre-trained Model\n\nDetect Kaggle vs local, load dataset, scaler, tokeniser, and the trained Phase-3 checkpoint.","metadata":{}},{"id":"c208bdf6","cell_type":"code","source":"# â”€â”€ Environment detection â”€â”€\nIS_KAGGLE = os.path.exists('/kaggle/input')\n\nif IS_KAGGLE:\n    DATA_PATH = '/kaggle/input/datasets/jasindavid/shifteddataset/label_shifted_fin_causality_dataset.csv'\n    MODEL_PATH = '/kaggle/working/models/multimodal_model_20260221_141142.pkl'\nelse:\n    DATA_PATH = r'D:\\NLP_ResearchPaper_work\\label_shifted_fin_causality_dataset.csv'\n    MODEL_PATH = r'D:\\NLP_ResearchPaper_work\\models\\multimodal_model_20260221_141142.pkl'\n\nprint(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Local'}\")\nprint(f\"Data path:   {DATA_PATH}\")\nprint(f\"Model path:  {MODEL_PATH}\")\n\n# â”€â”€ Load dataset â”€â”€\ndf = pd.read_csv(DATA_PATH)\nNUMERICAL_COLS = ['return_t1', 'return_t5', 'volatility_5']\n\n# Temporal 80/20 split (same as Phase 3)\nsplit_idx = int(len(df) * 0.8)\ntrain_df = df.iloc[:split_idx].reset_index(drop=True)\ntest_df  = df.iloc[split_idx:].reset_index(drop=True)\n\n# Fit RobustScaler on train, apply to both\nscaler = RobustScaler()\ntrain_df[NUMERICAL_COLS] = scaler.fit_transform(train_df[NUMERICAL_COLS])\ntest_df[NUMERICAL_COLS]  = scaler.transform(test_df[NUMERICAL_COLS])\n\nprint(f\"\\nTrain size: {len(train_df)} | Test size: {len(test_df)}\")\nprint(f\"Train label dist:\\n{train_df['label'].value_counts(normalize=True).to_string()}\")\nprint(f\"\\nTest label dist:\\n{test_df['label'].value_counts(normalize=True).to_string()}\")\n\n# â”€â”€ Tokeniser â”€â”€\nMODEL_NAME = 'ProsusAI/finbert'\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nMAX_LEN = 256\n\n# â”€â”€ Load pre-trained model â”€â”€\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nbase_model = AutoModel.from_pretrained(MODEL_NAME)\nmodel = EnhancedMultimodalFinBERT(base_model, num_numerical=3, dropout_rate=0.4).to(device)\n\ncheckpoint = torch.load(MODEL_PATH, map_location=device)\nif isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"\\nLoaded model from checkpoint (epoch {checkpoint.get('epoch','?')})\")\nelse:\n    model.load_state_dict(checkpoint)\n    print(\"\\nLoaded model state dict directly\")\n\nmodel.eval()\nprint(f\"Device: {device}\")\nprint(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"9285b73d","cell_type":"markdown","source":"## Section 4: Dataset & DataLoader","metadata":{}},{"id":"27bb5bcb","cell_type":"code","source":"class MultimodalDataset(Dataset):\n    \"\"\"Tokenised text + scaled numerical features + label.\"\"\"\n    def __init__(self, dataframe, tokenizer, max_len, numerical_cols):\n        self.texts = dataframe['clean_text'].fillna('').tolist()\n        self.numericals = dataframe[numerical_cols].values.astype(np.float32)\n        self.labels = dataframe['label'].values.astype(np.int64)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        enc = self.tokenizer(\n            self.texts[idx],\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'numerical': torch.tensor(self.numericals[idx], dtype=torch.float),\n            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# Build datasets\nBATCH_SIZE = 32\n\ntrain_dataset = MultimodalDataset(train_df, tokenizer, MAX_LEN, NUMERICAL_COLS)\ntest_dataset  = MultimodalDataset(test_df, tokenizer, MAX_LEN, NUMERICAL_COLS)\n\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n# For ablation training: split train into train/val (90/10)\nval_size = int(0.1 * len(train_dataset))\ntrain_size = len(train_dataset) - val_size\ntrain_subset, val_subset = torch.utils.data.random_split(\n    train_dataset, [train_size, val_size],\n    generator=torch.Generator().manual_seed(42)\n)\n\n# Weighted sampler for training (imbalanced classes)\ntrain_labels = [train_dataset.labels[i] for i in train_subset.indices]\nclass_counts = np.bincount(train_labels)\nbeta = 0.9999\neffective_num = 1.0 - np.power(beta, class_counts)\nclass_weights = (1.0 - beta) / (effective_num + 1e-8)\nsample_weights = [class_weights[l] for l in train_labels]\nsampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n\ntrain_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, sampler=sampler, num_workers=0)\nval_loader   = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nprint(f\"Train subset: {len(train_subset)} | Val subset: {len(val_subset)} | Test: {len(test_dataset)}\")\nprint(f\"Class distribution (train subset): {dict(zip(*np.unique(train_labels, return_counts=True)))}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"62993186","cell_type":"markdown","source":"## Section 5: Baseline Evaluation (Full Model)\n\nEvaluate the pre-trained Phase-3 multimodal model as the **baseline** for all comparisons.","metadata":{}},{"id":"0d244798","cell_type":"code","source":"def evaluate_model(model, loader, device, desc=\"Evaluating\"):\n    \"\"\"Evaluate any model variant and return metrics dict.\"\"\"\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=desc, leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            numerical = batch['numerical'].to(device)\n            labels = batch['label'].to(device)\n\n            logits = model(input_ids, attention_mask, numerical)\n            probs = torch.softmax(logits, dim=1)\n\n            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs[:, 1].cpu().numpy())\n\n    all_preds = np.array(all_preds)\n    all_labels = np.array(all_labels)\n    all_probs = np.array(all_probs)\n\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    try:\n        auc = roc_auc_score(all_labels, all_probs)\n    except ValueError:\n        auc = float('nan')\n\n    return {\n        'accuracy': acc,\n        'f1': f1,\n        'precision': prec,\n        'recall': rec,\n        'auc': auc,\n        'preds': all_preds,\n        'labels': all_labels,\n        'probs': all_probs\n    }\n\n# â”€â”€ Baseline evaluation â”€â”€\nprint(\"=\" * 60)\nprint(\"BASELINE â€” Full Multimodal Model (Phase 3)\")\nprint(\"=\" * 60)\nbaseline_results = evaluate_model(model, test_loader, device, desc=\"Baseline\")\n\nprint(f\"  Accuracy:  {baseline_results['accuracy']:.4f}\")\nprint(f\"  F1 Score:  {baseline_results['f1']:.4f}\")\nprint(f\"  Precision: {baseline_results['precision']:.4f}\")\nprint(f\"  Recall:    {baseline_results['recall']:.4f}\")\nprint(f\"  AUC:       {baseline_results['auc']:.4f}\")\nprint(f\"\\nClassification Report:\")\nprint(classification_report(baseline_results['labels'], baseline_results['preds'],\n                            target_names=['Non-Causal', 'Causal']))\n\n# Store all results for comparison\nablation_results = {'Full Multimodal (Baseline)': {\n    k: baseline_results[k] for k in ['accuracy', 'f1', 'precision', 'recall', 'auc']\n}}","metadata":{},"outputs":[],"execution_count":null},{"id":"055c9df1","cell_type":"markdown","source":"## Section 6: Feature Ablation (Zero-Out)\n\nMeasure the impact of each numerical feature by zeroing it out at inference time. A large drop indicates the feature is important.","metadata":{}},{"id":"7d732e87","cell_type":"code","source":"def evaluate_with_feature_ablation(model, loader, device, feature_idx_to_zero, feature_name):\n    \"\"\"Evaluate model with one numerical feature zeroed out.\"\"\"\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=f\"Ablate {feature_name}\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            numerical = batch['numerical'].clone().to(device)\n            labels = batch['label'].to(device)\n\n            # Zero out the target feature\n            numerical[:, feature_idx_to_zero] = 0.0\n\n            logits = model(input_ids, attention_mask, numerical)\n            probs = torch.softmax(logits, dim=1)\n\n            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs[:, 1].cpu().numpy())\n\n    all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    try:\n        auc = roc_auc_score(all_labels, all_probs)\n    except ValueError:\n        auc = float('nan')\n    return {'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': rec, 'auc': auc}\n\n\n# â”€â”€ Run feature ablation â”€â”€\nprint(\"=\" * 60)\nprint(\"FEATURE ABLATION â€” Zeroing Individual Numerical Features\")\nprint(\"=\" * 60)\n\nfeature_ablation_results = {}\nfor idx, col in enumerate(NUMERICAL_COLS):\n    res = evaluate_with_feature_ablation(model, test_loader, device, idx, col)\n    feature_ablation_results[col] = res\n    delta_f1 = res['f1'] - baseline_results['f1']\n    delta_acc = res['accuracy'] - baseline_results['accuracy']\n    print(f\"\\n  Without {col}:\")\n    print(f\"    Acc: {res['accuracy']:.4f} (Î”={delta_acc:+.4f})  F1: {res['f1']:.4f} (Î”={delta_f1:+.4f})  AUC: {res['auc']:.4f}\")\n\n# Also zero ALL numerical features\ndef evaluate_zero_all_numerical(model, loader, device):\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=\"Ablate ALL num\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            numerical = torch.zeros_like(batch['numerical']).to(device)\n            labels = batch['label'].to(device)\n            logits = model(input_ids, attention_mask, numerical)\n            probs = torch.softmax(logits, dim=1)\n            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs[:, 1].cpu().numpy())\n    all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n    try:\n        auc = roc_auc_score(all_labels, all_probs)\n    except ValueError:\n        auc = float('nan')\n    return {'accuracy': acc, 'f1': f1, 'precision': prec, 'recall': rec, 'auc': auc}\n\nres_zero_all = evaluate_zero_all_numerical(model, test_loader, device)\nfeature_ablation_results['ALL numerical'] = res_zero_all\ndelta_f1 = res_zero_all['f1'] - baseline_results['f1']\ndelta_acc = res_zero_all['accuracy'] - baseline_results['accuracy']\nprint(f\"\\n  Without ALL numerical features:\")\nprint(f\"    Acc: {res_zero_all['accuracy']:.4f} (Î”={delta_acc:+.4f})  F1: {res_zero_all['f1']:.4f} (Î”={delta_f1:+.4f})  AUC: {res_zero_all['auc']:.4f}\")\n\n# Store in main results\nfor col, res in feature_ablation_results.items():\n    ablation_results[f'w/o {col}'] = res","metadata":{},"outputs":[],"execution_count":null},{"id":"bcc0e143","cell_type":"markdown","source":"## Section 7: Training Utilities for Ablation Variants\n\nLightweight training loop with early stopping. We train ablation models for a limited number of epochs (ABLATION_EPOCHS = 5) to measure relative performance differences.","metadata":{}},{"id":"51344daa","cell_type":"code","source":"ABLATION_EPOCHS = 5\nPATIENCE = 2\n\nclass EarlyStopping:\n    def __init__(self, patience=2, min_delta=1e-4):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_score = None\n        self.counter = 0\n        self.should_stop = False\n\n    def __call__(self, score):\n        if self.best_score is None or score > self.best_score + self.min_delta:\n            self.best_score = score\n            self.counter = 0\n        else:\n            self.counter += 1\n            if self.counter >= self.patience:\n                self.should_stop = True\n\n\ndef train_ablation_model(model, train_loader, val_loader, device,\n                         epochs=ABLATION_EPOCHS, patience=PATIENCE, model_name=\"ablation\"):\n    \"\"\"Train an ablation variant with the same recipe as Phase 3 (simplified).\"\"\"\n    # Separate BERT and head parameters\n    bert_params, head_params = [], []\n    for name, param in model.named_parameters():\n        if 'bert' in name:\n            bert_params.append(param)\n        else:\n            head_params.append(param)\n\n    optimizer = torch.optim.AdamW([\n        {'params': bert_params, 'lr': 1e-5, 'weight_decay': 0.01},\n        {'params': head_params, 'lr': 5e-5, 'weight_decay': 0.01},\n    ])\n\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n    early_stop = EarlyStopping(patience=patience)\n\n    best_model_state = None\n    best_val_f1 = 0.0\n\n    for epoch in range(epochs):\n        # â”€â”€ Train â”€â”€\n        model.train()\n        total_loss, correct, total = 0, 0, 0\n        for batch in tqdm(train_loader, desc=f\"[{model_name}] Epoch {epoch+1}/{epochs}\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            numerical = batch['numerical'].to(device)\n            labels = batch['label'].to(device)\n\n            optimizer.zero_grad()\n            logits = model(input_ids, attention_mask, numerical)\n            loss = criterion(logits, labels)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            total_loss += loss.item() * labels.size(0)\n            correct += (logits.argmax(dim=1) == labels).sum().item()\n            total += labels.size(0)\n\n        scheduler.step()\n        train_loss = total_loss / total\n        train_acc = correct / total\n\n        # â”€â”€ Validate â”€â”€\n        model.eval()\n        val_preds, val_labels = [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                numerical = batch['numerical'].to(device)\n                labels = batch['label'].to(device)\n                logits = model(input_ids, attention_mask, numerical)\n                val_preds.extend(logits.argmax(dim=1).cpu().numpy())\n                val_labels.extend(labels.cpu().numpy())\n\n        val_f1 = f1_score(val_labels, val_preds, average='weighted')\n        val_acc = accuracy_score(val_labels, val_preds)\n\n        print(f\"  Epoch {epoch+1}: train_loss={train_loss:.4f}  train_acc={train_acc:.4f}  val_acc={val_acc:.4f}  val_f1={val_f1:.4f}\")\n\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}\n\n        early_stop(val_f1)\n        if early_stop.should_stop:\n            print(f\"  Early stopping at epoch {epoch+1}\")\n            break\n\n    # Restore best\n    if best_model_state:\n        model.load_state_dict(best_model_state)\n\n    return model, best_val_f1\n\nprint(f\"Training config: ABLATION_EPOCHS={ABLATION_EPOCHS}, PATIENCE={PATIENCE}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"f6248643","cell_type":"markdown","source":"## Section 8: Component Ablation â€” Train & Evaluate Variants\n\nTrain four architectural ablation variants:\n1. **Text-Only** â€” removes the numerical branch entirely\n2. **Numerical-Only** â€” removes FinBERT entirely (lightweight MLP on 3 features)\n3. **No Fusion Gate** â€” simple concatenation instead of gated fusion\n4. **CLS Pooling** â€” uses CLS token instead of learned attention pooling\n\nEach variant is trained for up to 5 epochs and evaluated on the same test set.","metadata":{}},{"id":"dcc291ef","cell_type":"code","source":"ablation_variants = {\n    'Text-Only': lambda: TextOnlyFinBERT(\n        AutoModel.from_pretrained(MODEL_NAME), dropout_rate=0.4\n    ),\n    'Numerical-Only': lambda: NumericalOnlyClassifier(\n        num_numerical=3, dropout_rate=0.4\n    ),\n    'No Fusion Gate': lambda: MultimodalNoGate(\n        AutoModel.from_pretrained(MODEL_NAME), num_numerical=3, dropout_rate=0.4\n    ),\n    'CLS Pooling': lambda: MultimodalCLSPooling(\n        AutoModel.from_pretrained(MODEL_NAME), num_numerical=3, dropout_rate=0.4\n    ),\n}\n\ncomponent_results = {}\n\nfor name, build_fn in ablation_variants.items():\n    print(\"\\n\" + \"=\" * 60)\n    print(f\"TRAINING: {name}\")\n    print(\"=\" * 60)\n\n    variant_model = build_fn().to(device)\n    param_count = sum(p.numel() for p in variant_model.parameters())\n    print(f\"  Parameters: {param_count:,}\")\n\n    variant_model, best_val_f1 = train_ablation_model(\n        variant_model, train_loader, val_loader, device, model_name=name\n    )\n    print(f\"  Best val F1: {best_val_f1:.4f}\")\n\n    # Evaluate on test set\n    test_res = evaluate_model(variant_model, test_loader, device, desc=f\"Test {name}\")\n    component_results[name] = {k: test_res[k] for k in ['accuracy', 'f1', 'precision', 'recall', 'auc']}\n    component_results[name]['params'] = param_count\n\n    delta_f1 = test_res['f1'] - baseline_results['f1']\n    delta_acc = test_res['accuracy'] - baseline_results['accuracy']\n    print(f\"  Test Acc: {test_res['accuracy']:.4f} (Î”={delta_acc:+.4f})\")\n    print(f\"  Test F1:  {test_res['f1']:.4f} (Î”={delta_f1:+.4f})\")\n    print(f\"  Test AUC: {test_res['auc']:.4f}\")\n\n    # Store in main results\n    ablation_results[name] = component_results[name]\n\n    # Free GPU memory\n    del variant_model\n    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n\nprint(\"\\nâœ“ All component ablation variants trained and evaluated.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"1c76c591","cell_type":"markdown","source":"## Section 9: Ablation Results Visualization","metadata":{}},{"id":"3112bf3d","cell_type":"code","source":"# â”€â”€ Ablation comparison bar chart â”€â”€\nresults_df = pd.DataFrame(ablation_results).T\nresults_df = results_df[['accuracy', 'f1', 'precision', 'recall', 'auc']]\nresults_df = results_df.sort_values('f1', ascending=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# F1 comparison\ncolors = ['#2ecc71' if idx == 'Full Multimodal (Baseline)' else '#e74c3c' if results_df.loc[idx, 'f1'] < baseline_results['f1'] else '#3498db'\n          for idx in results_df.index]\nbars = axes[0].barh(results_df.index, results_df['f1'], color=colors, edgecolor='black', linewidth=0.5)\naxes[0].axvline(x=baseline_results['f1'], color='green', linestyle='--', alpha=0.7, label='Baseline')\naxes[0].set_xlabel('Weighted F1 Score', fontsize=12)\naxes[0].set_title('Ablation Study â€” F1 Score Comparison', fontsize=14, fontweight='bold')\naxes[0].legend()\nfor bar, val in zip(bars, results_df['f1']):\n    axes[0].text(val + 0.002, bar.get_y() + bar.get_height()/2, f'{val:.4f}', va='center', fontsize=9)\n\n# Accuracy comparison\ncolors2 = ['#2ecc71' if idx == 'Full Multimodal (Baseline)' else '#e74c3c' if results_df.loc[idx, 'accuracy'] < baseline_results['accuracy'] else '#3498db'\n           for idx in results_df.index]\nbars2 = axes[1].barh(results_df.index, results_df['accuracy'], color=colors2, edgecolor='black', linewidth=0.5)\naxes[1].axvline(x=baseline_results['accuracy'], color='green', linestyle='--', alpha=0.7, label='Baseline')\naxes[1].set_xlabel('Accuracy', fontsize=12)\naxes[1].set_title('Ablation Study â€” Accuracy Comparison', fontsize=14, fontweight='bold')\naxes[1].legend()\nfor bar, val in zip(bars2, results_df['accuracy']):\n    axes[1].text(val + 0.002, bar.get_y() + bar.get_height()/2, f'{val:.4f}', va='center', fontsize=9)\n\nplt.tight_layout()\nplt.savefig('ablation_comparison.png', dpi=150, bbox_inches='tight')\nplt.show()\n\n# â”€â”€ Print summary table â”€â”€\nprint(\"\\nðŸ“Š Ablation Summary Table\")\nprint(\"=\" * 80)\nprint(results_df.to_string(float_format='{:.4f}'.format))\n\n# â”€â”€ Delta table (relative to baseline) â”€â”€\ndelta_df = results_df.copy()\nfor col in delta_df.columns:\n    delta_df[col] = delta_df[col] - ablation_results['Full Multimodal (Baseline)'][col]\n# Remove baseline row (all zeros)\ndelta_df = delta_df.drop('Full Multimodal (Baseline)', errors='ignore')\nprint(\"\\nðŸ“‰ Delta from Baseline\")\nprint(\"=\" * 80)\nprint(delta_df.to_string(float_format='{:+.4f}'.format))","metadata":{},"outputs":[],"execution_count":null},{"id":"c6c1d59e","cell_type":"markdown","source":"---\n# Part B: Robustness Analysis\n\n## Section 10: Text Perturbation Functions\n\nDefine four common text perturbation strategies to test model robustness:\n- **Word Dropout**: Randomly drop words\n- **Character Noise**: Swap random characters\n- **Word Shuffle**: Shuffle word order\n- **Synonym Replacement**: Replace words with synonyms (via WordNet)","metadata":{}},{"id":"33f5a21b","cell_type":"code","source":"import nltk\ntry:\n    from nltk.corpus import wordnet\n    wordnet.synsets('test')  # trigger download check\nexcept LookupError:\n    nltk.download('wordnet', quiet=True)\n    nltk.download('omw-1.4', quiet=True)\n    from nltk.corpus import wordnet\n\nimport random\nimport string\n\ndef word_dropout(text, rate=0.1):\n    \"\"\"Randomly drop words from text.\"\"\"\n    words = text.split()\n    if len(words) <= 1:\n        return text\n    kept = [w for w in words if random.random() > rate]\n    return ' '.join(kept) if kept else words[0]\n\ndef char_noise(text, rate=0.05):\n    \"\"\"Randomly replace characters with random letters.\"\"\"\n    chars = list(text)\n    for i in range(len(chars)):\n        if chars[i].isalpha() and random.random() < rate:\n            chars[i] = random.choice(string.ascii_lowercase)\n    return ''.join(chars)\n\ndef word_shuffle(text, rate=0.2):\n    \"\"\"Randomly swap adjacent words.\"\"\"\n    words = text.split()\n    n_swaps = max(1, int(len(words) * rate))\n    for _ in range(n_swaps):\n        if len(words) < 2:\n            break\n        idx = random.randint(0, len(words) - 2)\n        words[idx], words[idx + 1] = words[idx + 1], words[idx]\n    return ' '.join(words)\n\ndef synonym_replace(text, rate=0.1):\n    \"\"\"Replace random words with WordNet synonyms.\"\"\"\n    words = text.split()\n    new_words = []\n    for w in words:\n        if random.random() < rate:\n            synsets = wordnet.synsets(w)\n            if synsets:\n                synonyms = set()\n                for syn in synsets[:3]:\n                    for lemma in syn.lemmas():\n                        if lemma.name() != w and '_' not in lemma.name():\n                            synonyms.add(lemma.name())\n                if synonyms:\n                    new_words.append(random.choice(list(synonyms)))\n                    continue\n        new_words.append(w)\n    return ' '.join(new_words)\n\nprint(\"Text perturbation functions defined\")\n# Demo\nsample = \"Tesla stock plummeted after the earnings report disappointed investors\"\nprint(f\"\\nOriginal:   {sample}\")\nprint(f\"Word drop:  {word_dropout(sample, 0.3)}\")\nprint(f\"Char noise: {char_noise(sample, 0.1)}\")\nprint(f\"Word shuf:  {word_shuffle(sample, 0.3)}\")\nprint(f\"Synonym:    {synonym_replace(sample, 0.3)}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"d87ddcb8","cell_type":"markdown","source":"## Section 11: Text Perturbation Robustness Experiments\n\nApply each perturbation at increasing rates and measure F1 degradation.","metadata":{}},{"id":"f5559d35","cell_type":"code","source":"class PerturbedTextDataset(Dataset):\n    \"\"\"Apply a text perturbation function to the dataset on-the-fly.\"\"\"\n    def __init__(self, dataframe, tokenizer, max_len, numerical_cols, perturb_fn, perturb_rate):\n        self.texts = dataframe['clean_text'].fillna('').tolist()\n        self.numericals = dataframe[numerical_cols].values.astype(np.float32)\n        self.labels = dataframe['label'].values.astype(np.int64)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.perturb_fn = perturb_fn\n        self.perturb_rate = perturb_rate\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        text = self.perturb_fn(self.texts[idx], self.perturb_rate)\n        enc = self.tokenizer(text, max_length=self.max_len, padding='max_length',\n                             truncation=True, return_tensors='pt')\n        return {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0),\n            'numerical': torch.tensor(self.numericals[idx], dtype=torch.float),\n            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n        }\n\n# â”€â”€ Run text perturbation experiments â”€â”€\nperturbation_fns = {\n    'Word Dropout': word_dropout,\n    'Char Noise': char_noise,\n    'Word Shuffle': word_shuffle,\n    'Synonym Replace': synonym_replace,\n}\n\nperturbation_rates = [0.0, 0.05, 0.1, 0.2, 0.3, 0.5]\ntext_robustness_results = {}\n\nrandom.seed(42)\n\nfor fn_name, fn in perturbation_fns.items():\n    print(f\"\\n{'='*50}\")\n    print(f\"Text Perturbation: {fn_name}\")\n    print(f\"{'='*50}\")\n    rate_results = {}\n\n    for rate in perturbation_rates:\n        if rate == 0.0:\n            # Use baseline results\n            rate_results[rate] = {\n                'accuracy': baseline_results['accuracy'],\n                'f1': baseline_results['f1'],\n                'auc': baseline_results['auc']\n            }\n            print(f\"  Rate {rate:.2f}: F1={baseline_results['f1']:.4f} (baseline)\")\n            continue\n\n        perturbed_ds = PerturbedTextDataset(test_df, tokenizer, MAX_LEN, NUMERICAL_COLS, fn, rate)\n        perturbed_loader = DataLoader(perturbed_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n        res = evaluate_model(model, perturbed_loader, device, desc=f\"{fn_name} r={rate}\")\n        rate_results[rate] = {'accuracy': res['accuracy'], 'f1': res['f1'], 'auc': res['auc']}\n        delta = res['f1'] - baseline_results['f1']\n        print(f\"  Rate {rate:.2f}: F1={res['f1']:.4f} (Î”={delta:+.4f})  Acc={res['accuracy']:.4f}\")\n\n    text_robustness_results[fn_name] = rate_results\n\nprint(\"\\nâœ“ Text perturbation robustness analysis complete.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"cc340fa5","cell_type":"markdown","source":"## Section 12: Numerical Feature Robustness (Gaussian Noise)\n\nInject Gaussian noise into scaled numerical features at increasing standard deviations (Ïƒ) to test sensitivity.","metadata":{}},{"id":"e171bae5","cell_type":"code","source":"def evaluate_with_numerical_noise(model, loader, device, noise_std):\n    \"\"\"Evaluate model with Gaussian noise added to numerical features.\"\"\"\n    model.eval()\n    all_preds, all_labels, all_probs = [], [], []\n    with torch.no_grad():\n        for batch in tqdm(loader, desc=f\"Noise Ïƒ={noise_std:.1f}\", leave=False):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            numerical = batch['numerical'].to(device)\n            labels = batch['label'].to(device)\n\n            # Add Gaussian noise\n            noise = torch.randn_like(numerical) * noise_std\n            noisy_numerical = numerical + noise\n\n            logits = model(input_ids, attention_mask, noisy_numerical)\n            probs = torch.softmax(logits, dim=1)\n            all_preds.extend(logits.argmax(dim=1).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs[:, 1].cpu().numpy())\n\n    all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    try:\n        auc = roc_auc_score(all_labels, all_probs)\n    except ValueError:\n        auc = float('nan')\n    return {'accuracy': acc, 'f1': f1, 'auc': auc}\n\n# â”€â”€ Run numerical noise experiments â”€â”€\nnoise_stds = [0.0, 0.1, 0.25, 0.5, 1.0, 2.0, 3.0]\nnumerical_robustness = {}\n\nprint(\"=\" * 50)\nprint(\"NUMERICAL ROBUSTNESS â€” Gaussian Noise Injection\")\nprint(\"=\" * 50)\n\nfor sigma in noise_stds:\n    if sigma == 0.0:\n        numerical_robustness[sigma] = {\n            'accuracy': baseline_results['accuracy'],\n            'f1': baseline_results['f1'],\n            'auc': baseline_results['auc']\n        }\n        print(f\"  Ïƒ={sigma:.2f}: F1={baseline_results['f1']:.4f} (baseline)\")\n    else:\n        torch.manual_seed(42)\n        res = evaluate_with_numerical_noise(model, test_loader, device, sigma)\n        numerical_robustness[sigma] = res\n        delta = res['f1'] - baseline_results['f1']\n        print(f\"  Ïƒ={sigma:.2f}: F1={res['f1']:.4f} (Î”={delta:+.4f})  Acc={res['accuracy']:.4f}\")\n\nprint(\"\\nâœ“ Numerical noise robustness analysis complete.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"4a9106af","cell_type":"markdown","source":"## Section 13: Adversarial Robustness â€” FGSM Attack on Numerical Features\n\nApply Fast Gradient Sign Method (FGSM) to perturb numerical features in the direction that maximises loss.","metadata":{}},{"id":"bb8b03a6","cell_type":"code","source":"def fgsm_attack_numerical(model, loader, device, epsilon):\n    \"\"\"FGSM attack on numerical features only.\"\"\"\n    model.eval()\n    criterion = nn.CrossEntropyLoss()\n    all_preds, all_labels, all_probs = [], [], []\n\n    for batch in tqdm(loader, desc=f\"FGSM Îµ={epsilon:.2f}\", leave=False):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        numerical = batch['numerical'].to(device)\n        labels = batch['label'].to(device)\n\n        numerical.requires_grad_(True)\n\n        logits = model(input_ids, attention_mask, numerical)\n        loss = criterion(logits, labels)\n        loss.backward()\n\n        # FGSM perturbation\n        perturbation = epsilon * numerical.grad.sign()\n        adv_numerical = numerical + perturbation\n\n        with torch.no_grad():\n            adv_logits = model(input_ids, attention_mask, adv_numerical)\n            probs = torch.softmax(adv_logits, dim=1)\n            all_preds.extend(adv_logits.argmax(dim=1).cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_probs.extend(probs[:, 1].cpu().numpy())\n\n        numerical.requires_grad_(False)\n\n    all_preds, all_labels, all_probs = np.array(all_preds), np.array(all_labels), np.array(all_probs)\n    acc = accuracy_score(all_labels, all_preds)\n    f1 = f1_score(all_labels, all_preds, average='weighted')\n    try:\n        auc = roc_auc_score(all_labels, all_probs)\n    except ValueError:\n        auc = float('nan')\n    return {'accuracy': acc, 'f1': f1, 'auc': auc}\n\n# â”€â”€ Run FGSM attack at various epsilon levels â”€â”€\nepsilons = [0.0, 0.01, 0.05, 0.1, 0.25, 0.5, 1.0]\nfgsm_results = {}\n\nprint(\"=\" * 50)\nprint(\"ADVERSARIAL ROBUSTNESS â€” FGSM on Numerical Features\")\nprint(\"=\" * 50)\n\nfor eps in epsilons:\n    if eps == 0.0:\n        fgsm_results[eps] = {\n            'accuracy': baseline_results['accuracy'],\n            'f1': baseline_results['f1'],\n            'auc': baseline_results['auc']\n        }\n        print(f\"  Îµ={eps:.2f}: F1={baseline_results['f1']:.4f} (baseline)\")\n    else:\n        res = fgsm_attack_numerical(model, test_loader, device, eps)\n        fgsm_results[eps] = res\n        delta = res['f1'] - baseline_results['f1']\n        print(f\"  Îµ={eps:.2f}: F1={res['f1']:.4f} (Î”={delta:+.4f})  Acc={res['accuracy']:.4f}\")\n\nprint(\"\\nâœ“ FGSM adversarial robustness analysis complete.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"853391a8","cell_type":"markdown","source":"## Section 14: Temporal Distribution Shift\n\nSplit the test set by year to analyse how the model performs across different time periods (potential distribution shift).","metadata":{}},{"id":"498c6bec","cell_type":"code","source":"# â”€â”€ Extract year from date column (if available) â”€â”€\ndate_col = None\nfor col in ['date', 'Date', 'publish_date', 'published_date', 'timestamp']:\n    if col in df.columns:\n        date_col = col\n        break\n\ntemporal_results = {}\n\nif date_col:\n    test_df_orig = df.iloc[split_idx:].reset_index(drop=True)\n    test_df_orig['_year'] = pd.to_datetime(test_df_orig[date_col], errors='coerce').dt.year\n\n    print(\"=\" * 50)\n    print(\"TEMPORAL DISTRIBUTION SHIFT\")\n    print(\"=\" * 50)\n    print(f\"Date column: {date_col}\")\n    print(f\"Year distribution in test set:\")\n    print(test_df_orig['_year'].value_counts().sort_index().to_string())\n\n    for year in sorted(test_df_orig['_year'].dropna().unique()):\n        year = int(year)\n        mask = test_df_orig['_year'] == year\n        if mask.sum() < 20:\n            print(f\"  Year {year}: only {mask.sum()} samples â€” skipping\")\n            continue\n\n        year_df = test_df.loc[mask].reset_index(drop=True)\n        year_ds = MultimodalDataset(year_df, tokenizer, MAX_LEN, NUMERICAL_COLS)\n        year_loader = DataLoader(year_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n        res = evaluate_model(model, year_loader, device, desc=f\"Year {year}\")\n        temporal_results[year] = {'accuracy': res['accuracy'], 'f1': res['f1'], 'auc': res['auc'],\n                                  'n_samples': mask.sum()}\n        print(f\"  Year {year} (n={mask.sum()}): Acc={res['accuracy']:.4f}  F1={res['f1']:.4f}  AUC={res['auc']:.4f}\")\nelse:\n    print(\"âš  No date column found â€” skipping temporal analysis.\")\n    # Check if index can serve as proxy\n    print(\"  Available columns:\", list(df.columns[:10]))\n\nprint(\"\\nâœ“ Temporal distribution shift analysis complete.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"b6c26a9c","cell_type":"markdown","source":"## Section 15: Input Length Sensitivity\n\nBucket test samples by text length and evaluate per-bucket performance to detect length bias.","metadata":{}},{"id":"b18f6dfd","cell_type":"code","source":"# â”€â”€ Bucket test samples by word count â”€â”€\ntest_df['_word_count'] = test_df['clean_text'].fillna('').str.split().str.len()\nlength_bins = [0, 20, 50, 100, 150, 200, 500, float('inf')]\nlength_labels_str = ['<20', '20-50', '50-100', '100-150', '150-200', '200-500', '500+']\ntest_df['_length_bin'] = pd.cut(test_df['_word_count'], bins=length_bins, labels=length_labels_str, right=False)\n\nlength_results = {}\n\nprint(\"=\" * 50)\nprint(\"INPUT LENGTH SENSITIVITY\")\nprint(\"=\" * 50)\nprint(f\"Length distribution in test set:\")\nprint(test_df['_length_bin'].value_counts().sort_index().to_string())\nprint()\n\nfor bin_label in length_labels_str:\n    mask = test_df['_length_bin'] == bin_label\n    n_samples = mask.sum()\n    if n_samples < 20:\n        print(f\"  {bin_label} words (n={n_samples}): too few â€” skipping\")\n        continue\n\n    bin_df = test_df.loc[mask].reset_index(drop=True)\n    bin_ds = MultimodalDataset(bin_df, tokenizer, MAX_LEN, NUMERICAL_COLS)\n    bin_loader = DataLoader(bin_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\n    res = evaluate_model(model, bin_loader, device, desc=f\"Len {bin_label}\")\n    length_results[bin_label] = {\n        'accuracy': res['accuracy'], 'f1': res['f1'], 'auc': res['auc'], 'n_samples': n_samples\n    }\n    print(f\"  {bin_label} words (n={n_samples}): Acc={res['accuracy']:.4f}  F1={res['f1']:.4f}  AUC={res['auc']:.4f}\")\n\n# Cleanup temp columns\ntest_df.drop(columns=['_word_count', '_length_bin'], inplace=True, errors='ignore')\n\nprint(\"\\nâœ“ Input length sensitivity analysis complete.\")","metadata":{},"outputs":[],"execution_count":null},{"id":"66b09e54","cell_type":"markdown","source":"## Section 16: Robustness Sensitivity Curves\n\nFour-panel plot showing how F1 degrades under each perturbation type and intensity.","metadata":{}},{"id":"1fe023e0","cell_type":"code","source":"fig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig.suptitle('Robustness Sensitivity Analysis', fontsize=16, fontweight='bold', y=1.02)\n\n# â”€â”€ Panel 1: Text Perturbation â”€â”€\nax = axes[0, 0]\ncolors_text = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6']\nfor i, (fn_name, rate_res) in enumerate(text_robustness_results.items()):\n    rates = sorted(rate_res.keys())\n    f1s = [rate_res[r]['f1'] for r in rates]\n    ax.plot(rates, f1s, 'o-', label=fn_name, color=colors_text[i], linewidth=2, markersize=6)\nax.axhline(y=baseline_results['f1'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\nax.set_xlabel('Perturbation Rate', fontsize=12)\nax.set_ylabel('Weighted F1', fontsize=12)\nax.set_title('Text Perturbation Robustness', fontsize=13, fontweight='bold')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\nax.set_ylim(bottom=0)\n\n# â”€â”€ Panel 2: Numerical Noise â”€â”€\nax = axes[0, 1]\nsigmas = sorted(numerical_robustness.keys())\nf1s_noise = [numerical_robustness[s]['f1'] for s in sigmas]\nax.plot(sigmas, f1s_noise, 'o-', color='#e74c3c', linewidth=2, markersize=6, label='Gaussian Noise')\nax.axhline(y=baseline_results['f1'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\nax.fill_between(sigmas, f1s_noise, baseline_results['f1'], alpha=0.1, color='red')\nax.set_xlabel('Noise Ïƒ', fontsize=12)\nax.set_ylabel('Weighted F1', fontsize=12)\nax.set_title('Numerical Noise Robustness', fontsize=13, fontweight='bold')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\nax.set_ylim(bottom=0)\n\n# â”€â”€ Panel 3: FGSM Attack â”€â”€\nax = axes[1, 0]\neps_vals = sorted(fgsm_results.keys())\nf1s_fgsm = [fgsm_results[e]['f1'] for e in eps_vals]\nax.plot(eps_vals, f1s_fgsm, 's-', color='#8e44ad', linewidth=2, markersize=6, label='FGSM Attack')\nax.axhline(y=baseline_results['f1'], color='gray', linestyle='--', alpha=0.5, label='Baseline')\nax.fill_between(eps_vals, f1s_fgsm, baseline_results['f1'], alpha=0.1, color='purple')\nax.set_xlabel('FGSM Îµ', fontsize=12)\nax.set_ylabel('Weighted F1', fontsize=12)\nax.set_title('Adversarial Robustness (FGSM)', fontsize=13, fontweight='bold')\nax.legend(fontsize=9)\nax.grid(True, alpha=0.3)\nax.set_ylim(bottom=0)\n\n# â”€â”€ Panel 4: Input Length Sensitivity â”€â”€\nax = axes[1, 1]\nif length_results:\n    bins_sorted = list(length_results.keys())\n    f1s_len = [length_results[b]['f1'] for b in bins_sorted]\n    n_samples = [length_results[b]['n_samples'] for b in bins_sorted]\n    bars = ax.bar(bins_sorted, f1s_len, color='#3498db', edgecolor='black', linewidth=0.5, alpha=0.8)\n    ax.axhline(y=baseline_results['f1'], color='gray', linestyle='--', alpha=0.5, label='Overall Baseline')\n    # Annotate with sample counts\n    for bar, n, f in zip(bars, n_samples, f1s_len):\n        ax.text(bar.get_x() + bar.get_width()/2, f + 0.005, f'n={n}',\n                ha='center', va='bottom', fontsize=8)\n    ax.set_xlabel('Text Length (words)', fontsize=12)\n    ax.set_ylabel('Weighted F1', fontsize=12)\n    ax.set_title('Input Length Sensitivity', fontsize=13, fontweight='bold')\n    ax.legend(fontsize=9)\n    ax.grid(True, alpha=0.3, axis='y')\n    ax.set_ylim(bottom=0)\nelse:\n    ax.text(0.5, 0.5, 'No length data', transform=ax.transAxes, ha='center', fontsize=14)\n\nplt.tight_layout()\nplt.savefig('robustness_sensitivity_curves.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved: robustness_sensitivity_curves.png\")","metadata":{},"outputs":[],"execution_count":null},{"id":"a82c7c39","cell_type":"markdown","source":"## Section 17: Comprehensive Robustness Heatmap\n\nUnified heatmap showing F1 scores across all perturbation types and intensities.","metadata":{}},{"id":"4c5ef787","cell_type":"code","source":"# Build a unified heatmap matrix: rows = perturbation types, columns = intensity levels\n# Normalise intensities to common 0-5 scale for display\n\nheatmap_data = {}\n\n# Text perturbations\nfor fn_name, rate_res in text_robustness_results.items():\n    row = {}\n    for rate, res in sorted(rate_res.items()):\n        row[f'{rate:.0%}'] = res['f1']\n    heatmap_data[f'Text: {fn_name}'] = row\n\n# Numerical noise\nnum_row = {}\nfor sigma, res in sorted(numerical_robustness.items()):\n    num_row[f'Ïƒ={sigma}'] = res['f1']\nheatmap_data['Num: Gaussian Noise'] = num_row\n\n# FGSM\nfgsm_row = {}\nfor eps, res in sorted(fgsm_results.items()):\n    fgsm_row[f'Îµ={eps}'] = res['f1']\nheatmap_data['Adv: FGSM'] = fgsm_row\n\n# Create separate heatmaps for text and numerical/adversarial\nfig, axes = plt.subplots(3, 1, figsize=(14, 10))\n\n# Text perturbation heatmap\ntext_keys = [k for k in heatmap_data if k.startswith('Text')]\ntext_df = pd.DataFrame({k: heatmap_data[k] for k in text_keys}).T\ntext_df = text_df.astype(float)\nsns.heatmap(text_df, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[0],\n            vmin=0.5, vmax=1.0, linewidths=0.5, cbar_kws={'label': 'F1'})\naxes[0].set_title('Text Perturbation Robustness (F1 Scores)', fontsize=13, fontweight='bold')\naxes[0].set_ylabel('')\n\n# Numerical noise heatmap\nnum_df = pd.DataFrame({'Num: Gaussian Noise': heatmap_data['Num: Gaussian Noise']}).T\nnum_df = num_df.astype(float)\nsns.heatmap(num_df, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[1],\n            vmin=0.5, vmax=1.0, linewidths=0.5, cbar_kws={'label': 'F1'})\naxes[1].set_title('Numerical Noise Robustness (F1 Scores)', fontsize=13, fontweight='bold')\naxes[1].set_ylabel('')\n\n# FGSM heatmap\nfgsm_df = pd.DataFrame({'Adv: FGSM': heatmap_data['Adv: FGSM']}).T\nfgsm_df = fgsm_df.astype(float)\nsns.heatmap(fgsm_df, annot=True, fmt='.3f', cmap='RdYlGn', ax=axes[2],\n            vmin=0.5, vmax=1.0, linewidths=0.5, cbar_kws={'label': 'F1'})\naxes[2].set_title('Adversarial FGSM Robustness (F1 Scores)', fontsize=13, fontweight='bold')\naxes[2].set_ylabel('')\n\nplt.tight_layout()\nplt.savefig('robustness_heatmap.png', dpi=150, bbox_inches='tight')\nplt.show()\nprint(\"Saved: robustness_heatmap.png\")","metadata":{},"outputs":[],"execution_count":null},{"id":"fb1a6421","cell_type":"markdown","source":"## Section 18: Summary Results Table & CSV Export","metadata":{}},{"id":"02df29a5","cell_type":"code","source":"# â”€â”€ Build comprehensive summary DataFrame â”€â”€\nsummary_rows = []\n\n# 1. Ablation results (baseline + component + feature)\nfor name, metrics in ablation_results.items():\n    row = {'Experiment': name, 'Category': 'Ablation'}\n    row.update({k: metrics[k] for k in ['accuracy', 'f1', 'precision', 'recall', 'auc'] if k in metrics})\n    baseline_f1 = ablation_results['Full Multimodal (Baseline)']['f1']\n    row['delta_f1'] = metrics['f1'] - baseline_f1\n    summary_rows.append(row)\n\n# 2. Text perturbation (worst-case for each)\nfor fn_name, rate_res in text_robustness_results.items():\n    worst_rate = max(rate_res.keys())\n    worst = rate_res[worst_rate]\n    row = {\n        'Experiment': f'Text: {fn_name} (rate={worst_rate})',\n        'Category': 'Text Robustness',\n        'accuracy': worst['accuracy'],\n        'f1': worst['f1'],\n        'auc': worst['auc'],\n        'delta_f1': worst['f1'] - baseline_f1\n    }\n    summary_rows.append(row)\n\n# 3. Numerical noise (worst-case)\nworst_sigma = max(numerical_robustness.keys())\nworst_num = numerical_robustness[worst_sigma]\nsummary_rows.append({\n    'Experiment': f'Num Noise (Ïƒ={worst_sigma})',\n    'Category': 'Numerical Robustness',\n    'accuracy': worst_num['accuracy'],\n    'f1': worst_num['f1'],\n    'auc': worst_num['auc'],\n    'delta_f1': worst_num['f1'] - baseline_f1\n})\n\n# 4. FGSM (worst-case)\nworst_eps = max(fgsm_results.keys())\nworst_fgsm = fgsm_results[worst_eps]\nsummary_rows.append({\n    'Experiment': f'FGSM (Îµ={worst_eps})',\n    'Category': 'Adversarial',\n    'accuracy': worst_fgsm['accuracy'],\n    'f1': worst_fgsm['f1'],\n    'auc': worst_fgsm['auc'],\n    'delta_f1': worst_fgsm['f1'] - baseline_f1\n})\n\n# 5. Temporal (if available)\nfor year, res in temporal_results.items():\n    summary_rows.append({\n        'Experiment': f'Temporal: Year {year}',\n        'Category': 'Temporal Shift',\n        'accuracy': res['accuracy'],\n        'f1': res['f1'],\n        'auc': res['auc'],\n        'delta_f1': res['f1'] - baseline_f1\n    })\n\nsummary_df = pd.DataFrame(summary_rows)\nsummary_df = summary_df.sort_values('delta_f1', ascending=True)\n\n# Display\nprint(\"=\" * 90)\nprint(\"COMPREHENSIVE ABLATION & ROBUSTNESS SUMMARY\")\nprint(\"=\" * 90)\ndisplay_cols = ['Experiment', 'Category', 'accuracy', 'f1', 'auc', 'delta_f1']\navailable_cols = [c for c in display_cols if c in summary_df.columns]\nprint(summary_df[available_cols].to_string(index=False, float_format='{:.4f}'.format))\n\n# Export to CSV\ncsv_path = 'phase5_ablation_robustness_results.csv'\nsummary_df.to_csv(csv_path, index=False)\nprint(f\"\\nâœ“ Results saved to {csv_path}\")","metadata":{},"outputs":[],"execution_count":null},{"id":"66d7c196","cell_type":"markdown","source":"## Section 19: Final Report & Key Findings","metadata":{}},{"id":"5c9dfdea","cell_type":"code","source":"# â”€â”€ Automated report generation â”€â”€\nbaseline_f1 = ablation_results['Full Multimodal (Baseline)']['f1']\nbaseline_acc = ablation_results['Full Multimodal (Baseline)']['accuracy']\n\nprint(\"=\" * 70)\nprint(\"  PHASE 5 â€” ABLATION & ROBUSTNESS ANALYSIS: KEY FINDINGS\")\nprint(\"=\" * 70)\n\nprint(f\"\"\"\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  BASELINE PERFORMANCE                                           â”‚\nâ”‚  Accuracy: {baseline_acc:.4f}   F1: {baseline_f1:.4f}                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\"\"\")\n\n# â”€â”€ Component Ablation Summary â”€â”€\nprint(\"1. COMPONENT ABLATION RESULTS\")\nprint(\"â”€\" * 50)\nfor name in ['Text-Only', 'Numerical-Only', 'No Fusion Gate', 'CLS Pooling']:\n    if name in ablation_results:\n        r = ablation_results[name]\n        delta = r['f1'] - baseline_f1\n        print(f\"   {name:25s} F1={r['f1']:.4f}  (Î”={delta:+.4f})\")\n\n# â”€â”€ Feature Importance (from ablation) â”€â”€\nprint(f\"\\n2. FEATURE IMPORTANCE (Zero-Out Ablation)\")\nprint(\"â”€\" * 50)\nfor col in NUMERICAL_COLS + ['ALL numerical']:\n    key = f'w/o {col}'\n    if key in ablation_results:\n        r = ablation_results[key]\n        delta = r['f1'] - baseline_f1\n        importance = abs(delta)\n        bar = 'â–ˆ' * int(importance * 200)\n        print(f\"   {col:20s}  Î”_F1={delta:+.4f}  {bar}\")\n\n# â”€â”€ Robustness Summary â”€â”€\nprint(f\"\\n3. ROBUSTNESS ANALYSIS SUMMARY\")\nprint(\"â”€\" * 50)\n\n# Find worst text perturbation\nworst_text_name, worst_text_f1 = '', 1.0\nfor fn_name, rate_res in text_robustness_results.items():\n    worst_rate = max(rate_res.keys())\n    if rate_res[worst_rate]['f1'] < worst_text_f1:\n        worst_text_f1 = rate_res[worst_rate]['f1']\n        worst_text_name = f\"{fn_name} (rate={worst_rate})\"\n\nprint(f\"   Worst text perturbation:  {worst_text_name}\")\nprint(f\"     â†’ F1 dropped to {worst_text_f1:.4f} (Î”={worst_text_f1-baseline_f1:+.4f})\")\n\nworst_sigma = max(numerical_robustness.keys())\nworst_num_f1 = numerical_robustness[worst_sigma]['f1']\nprint(f\"   Worst num noise (Ïƒ={worst_sigma}):    F1={worst_num_f1:.4f} (Î”={worst_num_f1-baseline_f1:+.4f})\")\n\nworst_eps = max(fgsm_results.keys())\nworst_fgsm_f1 = fgsm_results[worst_eps]['f1']\nprint(f\"   Worst FGSM (Îµ={worst_eps}):        F1={worst_fgsm_f1:.4f} (Î”={worst_fgsm_f1-baseline_f1:+.4f})\")\n\nif temporal_results:\n    min_year = min(temporal_results, key=lambda y: temporal_results[y]['f1'])\n    max_year = max(temporal_results, key=lambda y: temporal_results[y]['f1'])\n    print(f\"   Best temporal period:     Year {max_year} (F1={temporal_results[max_year]['f1']:.4f})\")\n    print(f\"   Worst temporal period:    Year {min_year} (F1={temporal_results[min_year]['f1']:.4f})\")\n\n# â”€â”€ Conclusions â”€â”€\nprint(f\"\"\"\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  KEY CONCLUSIONS                                                 â”‚\nâ”‚                                                                  â”‚\nâ”‚  â€¢ The multimodal architecture (text + numerical + gate)         â”‚\nâ”‚    justifies its design if it outperforms both single-modality   â”‚\nâ”‚    baselines in F1.                                              â”‚\nâ”‚                                                                  â”‚\nâ”‚  â€¢ The fusion gate's contribution is measured by comparing       â”‚\nâ”‚    'Full Multimodal' vs 'No Fusion Gate'.                        â”‚\nâ”‚                                                                  â”‚\nâ”‚  â€¢ Attention pooling's value is measured by comparing            â”‚\nâ”‚    'Full Multimodal' vs 'CLS Pooling'.                           â”‚\nâ”‚                                                                  â”‚\nâ”‚  â€¢ Feature ablation reveals which numerical signals              â”‚\nâ”‚    (return_t1, return_t5, volatility_5) matter most.             â”‚\nâ”‚                                                                  â”‚\nâ”‚  â€¢ Text robustness shows how resilient the model is to           â”‚\nâ”‚    real-world noise (typos, missing words, paraphrasing).        â”‚\nâ”‚                                                                  â”‚\nâ”‚  â€¢ FGSM results quantify adversarial vulnerability â€” critical    â”‚\nâ”‚    for financial applications requiring trustworthiness.          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\"\"\")\n\nprint(\"=\" * 70)\nprint(\"  Phase 5 â€” Ablation & Robustness Analysis COMPLETE\")\nprint(\"=\" * 70)\nprint(f\"\\nOutput files:\")\nprint(f\"  â€¢ ablation_comparison.png\")\nprint(f\"  â€¢ robustness_sensitivity_curves.png\")\nprint(f\"  â€¢ robustness_heatmap.png\")\nprint(f\"  â€¢ phase5_ablation_robustness_results.csv\")","metadata":{},"outputs":[],"execution_count":null}]}